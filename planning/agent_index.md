Title: Sapflux agent index and source-of-truth map

Purpose

This short index helps coding agents find the right files fast, keep context small, and resolve conflicts between docs and code without needing frequent updates to this index.
When in doubt, trust the code and tests in crates/ over any planning document. Use planning docs only for intent and background.

Conflict resolution

If code/tests and docs disagree: code + tests are authoritative.
Use the latest progress report and latest code review for “what’s next” (see below). Otherwise, use the module map and tests to infer intended behavior.
How to find the latest status and review (auto-updating rule)

Latest progress report: open progress_report/statusX.md with the highest numeric X. Example: status16.md is newer than status15.md.
Latest code review: open the file with the highest leading number in code_reviews/. Compare the leading integer only.
Minimal context set (what to load by default)

Code reviews come from an external source and Riley Leff will add them at his discretion.
Progress reports are generated by the implementing agent and are kept in the progress_report folder. You can generate one every time you make meaningful progress in the implementation.
Always include:

crates/** (all code and tests)
crates/sapflux-core/migrations/0001_init.sql
This file: planning/agent_index.md
Omit unless you are modifying them or need background:

planning/reference_implementations/**
code_reviews/** (read only the highest-numbered one when checking current feedback)
progress_report/** (read only the highest-numbered one when checking current status)
planning/writeups/** (optional; see “Planning docs map” below)
planning/notes/** (optional, topic-specific background)
Repository map (where things live)

API server (Axum) and CLI entry
crates/sapflux/src/main.rs
Routes of note: POST /transactions (multipart), POST /admin/migrate, POST /admin/seed, GET /outputs/:id/download
Admin tooling
crates/sapflux-admin/src/main.rs (db seed, object-store GC)
Database and migrations
crates/sapflux-core/src/db/mod.rs
crates/sapflux-core/migrations/0001_init.sql
crates/sapflux-core/src/seed.rs (seeds data_formats, parsers, pipelines, parameters)
Object storage abstraction
crates/sapflux-core/src/object_store.rs (LocalDir + S3/R2 client; presign/list/delete; env-configurable)
Gated S3 smoke test: crates/sapflux/tests/object_store_s3.rs
Object-store GC
crates/sapflux-core/src/object_gc.rs (plan/apply GC)
sapflux-admin object-store-gc (dry-run by default; --apply to delete)
Parsers
Implementations: crates/sapflux-parser/**
Core wrappers/registry: crates/sapflux-core/src/parsers.rs
Ingestion
crates/sapflux-core/src/ingestion.rs (hashing, duplicate detection, parser attempts, reports)
Tests: crates/sapflux-core/tests/ingestion.rs
Data flattening (hierarchical → observation rows)
crates/sapflux-core/src/flatten.rs
Tests: crates/sapflux-core/tests/flatten.rs
Timestamp fixer (implied-visit algorithm with DST)
crates/sapflux-core/src/timestamp_fixer.rs
Tests: crates/sapflux-core/tests/timestamp_fixer.rs
Metadata enrichment (deployments, hierarchy IDs, installation_metadata expansion, alias resolution)
crates/sapflux-core/src/metadata_enricher.rs
Tests: crates/sapflux-core/tests/metadata_enricher.rs
Parameter cascade (defaults, overrides, provenance)
crates/sapflux-core/src/parameter_resolver.rs
Tests: crates/sapflux-core/tests/parameter_resolver.rs
Calculator (DMA Péclet; HRM/Tmax parallel outputs + switch)
crates/sapflux-core/src/calculator.rs
Tests: crates/sapflux-core/tests/calculator.rs
Quality filters (canonical rules + explanations)
crates/sapflux-core/src/quality_filters.rs
Tests: crates/sapflux-core/tests/quality_filters.rs
Processing pipelines (batch orchestration within a run)
crates/sapflux-core/src/pipelines.rs (standard_v1_dst_fix pipeline)
Transactions and receipts
crates/sapflux-core/src/transactions.rs
Integration test: crates/sapflux/tests/transactions.rs
Metadata manifests (TOML)
crates/sapflux-core/src/metadata_manifest.rs (parse, preflight validation, transactional apply)
Outputs (publish + download)
crates/sapflux-core/src/outputs.rs (serialize DataFrame to Parquet, build cartridge, upload-first, persist runs/outputs, flip is_latest)
API download endpoint: crates/sapflux/src/main.rs (GET /outputs/:id/download; include_cartridge flag)
Local dev stack (compose + smoke)

Compose harness: docker-compose.yaml (db + MinIO + API)
One-command smoke: integration_tests/smoke.sh (migrate/seed → multipart transaction → publish → presigned download and save parquet under integration_tests/output)
Dev notes: docs/dev-compose.md
Data flow overview (end-to-end)

POST /transactions (multipart/form-data: manifest + files[])

Advisory lock serializes execution.
If present, parse TOML metadata manifest; preflight validate selectors, ranges, uniqueness. On commit, apply manifest in a single DB transaction.
Ingestion: stream each file, compute blake3, detect duplicates, parse via active parsers, collect per-file reports and parser_attempts (with first_error_line when available).
Batch pipeline on all successfully parsed files:
flatten → timestamp_fixer → metadata_enricher → parameter_resolver → calculator → quality_filters
Object-store upload-first for new raw files (idempotent; LocalDir or S3/R2 depending on env).
If pipeline OK and uploads OK:
Persist raw_files rows
Publish output artifacts:
Serialize final DataFrame → Parquet; upload-first to outputs/{output_id}.parquet
Build reproducibility cartridge; upload-first to repro-cartridges/{output_id}.zip
In one DB tx: insert runs row and outputs row; flip all prior outputs.is_latest to false
Update transactions row with outcome (ACCEPTED/REJECTED) and final receipt.
Dry run: no DB/object-store writes; return full receipt (uses current DB state for pipeline).
Download API

GET /outputs/{id}/download returns a presigned URL (JSON) to private object store
Query param include_cartridge=true returns the cartridge instead of the parquet
Database quick facts

Schema: crates/sapflux-core/migrations/0001_init.sql
Transactions: immutable audit with outcome PENDING → ACCEPTED/REJECTED and stored receipt
raw_files(file_hash) references the transaction that ingested it
deployments and datalogger_aliases use exclusion constraints to prevent overlaps and adjacency
outputs.is_latest: insert new output with is_latest=true and flip others to false in the same transaction
Object storage keys and policy

raw-files/{file_hash} (content-addressed raw files)
outputs/{output_id}.parquet
repro-cartridges/{output_id}.zip
Upload-first pattern; orphan objects are GC candidates (GC dry-run/confirm available)
Receipts

Per-file reports with parser_attempts and first_error_line when available
ingestion_summary totals (total/parsed/duplicates/failed)
pipeline summary (status, row_count, error if any), plus:
quality_summary (row counts, suspect ratio, top reasons)
provenance_summary (top non-default override sources)
record_summary (logger_count, sensor_count, timeframe_utc)
On ACCEPTED, artifacts keys included (parquet and cartridge keys); clients may presign via download endpoint
Testing guidance

Unit tests for each component live in crates/sapflux-core/tests/**
Transactions integration test requires SAPFLUX_TEST_DATABASE_URL; skips otherwise
S3/R2 smoke test (env-gated): crates/sapflux/tests/object_store_s3.rs
Local end-to-end: docker-compose.yaml + scripts/smoke.sh
Planning docs map (optional reading)

planning/plan.md: high-level implementation plan
planning/writeups/02_database_and_storage.md: DB + object store overview
planning/writeups/03_transaction_workflow.md: transaction lifecycle and receipts
planning/writeups/05_processing_and_calculations.md: pipeline stage order and roles
planning/writeups/09_cartridge.md: reproducibility cartridge goals and contents
planning/notes/*.md: topic deep-dives (read only if needed)
planning/reference_implementations/**: reference-only; do not rely on these over code
How to keep this index evergreen without edits

For current status/tasks: open the highest-numbered file in:
progress_report/ for the latest statusX.md
code_reviews/ for the latest N_*.md
This index only points to locations; you can append new reports/reviews without modifying this file.
Lightweight prompt recipe (to keep tokens small)

Always include:

crates/** and migrations/0001_init.sql
planning/agent_index.md Only when needed:
Highest-numbered progress_report/statusX.md
Highest-numbered code_reviews/N_*.md
One or two writeups for background (02/03/05), not the whole planning folder
Common conventions

Columns:
logger_id is canonical and stable per file; parsers enforce single-ID invariant
timestamp_utc is timezone-aware UTC
parameter_* columns include parameter_source_* provenance
Quality outputs: quality and quality_explanation (pipe-delimited reasons)
Grouping and dedup:
Timestamp fixer dedupes by (logger_id, record) across files and uses a sorted file_set_signature to define implied visits
If you need “what’s next”

Read the latest code review and latest progress report per the rules above
End of agent index.
