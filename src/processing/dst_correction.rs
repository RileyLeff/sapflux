use crate::types::{DstTransitionTable, RawDataFile};
use polars::prelude::*;
use polars::time::*;
use chrono::{DateTime, Utc, TimeZone};
use std::collections::HashMap;
use std::path::PathBuf;

pub struct DstCorrector {
    transition_table: DstTransitionTable,
}

#[derive(Debug, Clone)]
pub struct DataChunk {
    pub origin_files: Vec<PathBuf>,
    pub first_timestamp: DateTime<Utc>,
    pub last_timestamp: DateTime<Utc>,
    pub determined_timezone_offset: i32, // Hours from UTC (-5 for EST, -4 for EDT)
    pub data_point_count: usize,
}

impl DstCorrector {
    pub fn new() -> Self {
        Self {
            transition_table: DstTransitionTable::us_eastern_2011_2030(),
        }
    }
    
    /// Full DST correction algorithm as described in README
    pub fn correct_timestamps_full(
        &self,
        df: LazyFrame,
        raw_files: &[RawDataFile],
    ) -> PolarsResult<LazyFrame> {
        println!("üïê Starting full DST correction algorithm...");
        
        // Step 1: Identify unique data chunks based on file origins
        let chunks = self.identify_data_chunks(df.clone(), raw_files)?;
        println!("üìÅ Identified {} unique data chunks", chunks.len());
        
        // Step 2: Process each chunk with its determined timezone
        let mut corrected_chunks = Vec::new();
        
        for (i, chunk) in chunks.iter().enumerate() {
            println!("üîç Processing chunk {} ({} data points)", i + 1, chunk.data_point_count);
            println!("   Files: {:?}", chunk.origin_files.iter().map(|p| p.file_name().unwrap_or_default()).collect::<Vec<_>>());
            println!("   Time range: {} to {}", chunk.first_timestamp, chunk.last_timestamp);
            println!("   Determined timezone: UTC{:+}", chunk.determined_timezone_offset);
            
            // Apply timezone correction to this chunk
            let chunk_corrected = self.apply_timezone_correction(df.clone(), chunk)?;
            corrected_chunks.push(chunk_corrected);
        }
        
        // Step 3: Combine all corrected chunks
        let final_corrected = self.combine_corrected_chunks(corrected_chunks)?;
        
        // Step 4: Validation - check for DST transition issues
        let validated = self.validate_dst_transitions(final_corrected)?;
        
        println!("‚úÖ DST correction completed successfully");
        Ok(validated)
    }
    
    /// Simplified version for compatibility (delegates to full version)
    pub fn correct_timestamps(
        &self,
        df: LazyFrame,
        _chunk_identifier: &str,
    ) -> PolarsResult<LazyFrame> {
        // For now, apply a basic correction without full chunk detection
        // This maintains compatibility while the full algorithm is being integrated
        let corrected = df
            .with_columns([
                lit("partial_dst_correction").alias("dst_status"),
                lit(-5).alias("assumed_timezone_offset_hours"), // EST = UTC-5 baseline
                
                // Add validation flags
                col("timestamp")
                    .gt_eq(lit(chrono::Utc.with_ymd_and_hms(2021, 1, 1, 0, 0, 0).unwrap().timestamp_millis()))
                    .and(col("timestamp").lt_eq(lit(chrono::Utc::now().timestamp_millis())))
                    .alias("timestamp_valid"),
            ])
            .filter(col("timestamp_valid").eq(lit(true)))
            .drop(["timestamp_valid"]);
            
        Ok(corrected)
    }
    
    /// Step 1: Identify unique data chunks based on file origins and data collection periods
    /// CRITICAL: Chunks represent periods when logger was set to one timezone (do NOT split at DST transitions)
    fn identify_data_chunks(
        &self,
        df: LazyFrame,
        raw_files: &[RawDataFile],
    ) -> PolarsResult<Vec<DataChunk>> {
        let _collected = df.collect()?;
        
        // Create a map of timestamps to their origin files
        let mut timestamp_to_files: HashMap<i64, Vec<PathBuf>> = HashMap::new();
        
        for raw_file in raw_files {
            for data_point in &raw_file.data_points {
                let timestamp_ms = data_point.timestamp.timestamp_millis();
                timestamp_to_files
                    .entry(timestamp_ms)
                    .or_default()
                    .push(raw_file.file_path.clone());
            }
        }
        
        // Group timestamps by unique file combinations to identify initial chunks
        let mut file_combo_to_timestamps: HashMap<Vec<PathBuf>, Vec<i64>> = HashMap::new();
        
        for (timestamp, files) in timestamp_to_files {
            let mut sorted_files = files;
            sorted_files.sort();
            sorted_files.dedup();
            
            file_combo_to_timestamps
                .entry(sorted_files)
                .or_default()
                .push(timestamp);
        }
        
        // Convert to DataChunk objects and split at DST transitions
        let mut chunks = Vec::new();
        
        for (files, mut timestamps) in file_combo_to_timestamps {
            timestamps.sort();
            
            if timestamps.is_empty() {
                continue;
            }
            
            // Create a natural chunk for this file combination (NO DST splitting)
            let sub_chunks = self.create_natural_chunk(files, timestamps)?;
            chunks.extend(sub_chunks);
        }
        
        // Sort chunks by first timestamp
        chunks.sort_by_key(|c| c.first_timestamp);
        
        Ok(chunks)
    }
    
    /// Create natural data chunks based on file origins and data collection periods
    /// CRITICAL: Chunks should NOT be split at DST transitions!
    /// Each chunk represents a period where the logger was set to one timezone configuration
    fn create_natural_chunk(
        &self,
        files: Vec<PathBuf>,
        timestamps: Vec<i64>,
    ) -> PolarsResult<Vec<DataChunk>> {
        if timestamps.is_empty() {
            return Ok(Vec::new());
        }
        
        // Sort timestamps to ensure chronological order
        let mut timestamps = timestamps;
        timestamps.sort();
        
        let first_ts = timestamps[0];
        let last_ts = timestamps[timestamps.len() - 1];
        
        let first_datetime = DateTime::<Utc>::from_timestamp_millis(first_ts)
            .unwrap_or_else(|| Utc::now());
        let last_datetime = DateTime::<Utc>::from_timestamp_millis(last_ts)
            .unwrap_or_else(|| Utc::now());
        
        // Determine the timezone the logger was configured for during this ENTIRE period
        // Use the FIRST timestamp to determine logger configuration
        let chunk_timezone_offset = self.determine_chunk_timezone(first_datetime);
        
        println!("üìÅ Creating chunk from {} to {}", first_datetime, last_datetime);
        println!("   Determined logger was set to: UTC{:+} for ENTIRE period", chunk_timezone_offset);
        println!("   (No DST splits - logger doesn't auto-adjust timezone)");
        
        // Create a single chunk for this entire period
        let chunk = DataChunk {
            origin_files: files,
            first_timestamp: first_datetime,
            last_timestamp: last_datetime,
            determined_timezone_offset: chunk_timezone_offset,
            data_point_count: timestamps.len(),
        };
        
        Ok(vec![chunk])
    }
    
    /// Create a DataChunk from a slice of timestamps
    fn create_chunk_from_timestamps(
        &self,
        files: Vec<PathBuf>,
        timestamps: &[i64],
        timezone_offset: i32,
    ) -> PolarsResult<Option<DataChunk>> {
        if timestamps.is_empty() {
            return Ok(None);
        }
        
        let first_ts = timestamps[0];
        let last_ts = timestamps[timestamps.len() - 1];
        
        let first_datetime = DateTime::<Utc>::from_timestamp_millis(first_ts)
            .unwrap_or_else(|| Utc::now());
        let last_datetime = DateTime::<Utc>::from_timestamp_millis(last_ts)
            .unwrap_or_else(|| Utc::now());
        
        let chunk = DataChunk {
            origin_files: files,
            first_timestamp: first_datetime,
            last_timestamp: last_datetime,
            determined_timezone_offset: timezone_offset,
            data_point_count: timestamps.len(),
        };
        
        Ok(Some(chunk))
    }
    
    /// Step 2: Determine timezone for a chunk based on its first timestamp
    fn determine_chunk_timezone(&self, first_timestamp: DateTime<Utc>) -> i32 {
        // The first timestamp in a chunk represents when the logger was last synced
        // We need to determine if the logger was set to EST or EDT at that time
        
        // Convert to naive datetime since the timestamp should be treated as naive local time
        let naive_local_time = first_timestamp.naive_utc();
        let offset = self.transition_table.determine_timezone_offset(naive_local_time);
        offset
    }
    
    /// Step 3: Apply timezone correction to a specific chunk
    fn apply_timezone_correction(
        &self,
        df: LazyFrame,
        chunk: &DataChunk,
    ) -> PolarsResult<LazyFrame> {
        let collected = df.collect()?;
        
        // Filter data points that belong to this chunk based on timestamp range
        let chunk_data = collected
            .lazy()
            .filter(
                col("timestamp").gt_eq(lit(chunk.first_timestamp.timestamp_millis()))
                    .and(col("timestamp").lt_eq(lit(chunk.last_timestamp.timestamp_millis())))
            )
            .with_columns([
                // Keep original timestamp for reference
                col("timestamp").alias("timestamp_original_local"),
                    
                // Apply the actual timezone correction using duration 
                // Create a duration from hours and add it to timestamp
                (col("timestamp") + duration(DurationArgs {
                    hours: lit((-chunk.determined_timezone_offset) as i64),
                    ..Default::default()
                }))
                    .alias("timestamp_utc_corrected"),
                    
                lit(chunk.determined_timezone_offset).alias("original_timezone_offset"),
                lit("dst_corrected").alias("dst_status"),
                
                // Add chunk identification for debugging
                lit(format!("chunk_{}", chunk.first_timestamp.format("%Y%m%d_%H%M%S")))
                    .alias("chunk_id"),
            ]);
            
        Ok(chunk_data)
    }
    
    /// Step 4: Combine all corrected chunks
    fn combine_corrected_chunks(
        &self,
        chunks: Vec<LazyFrame>,
    ) -> PolarsResult<LazyFrame> {
        if chunks.is_empty() {
            return Err(PolarsError::ComputeError("No chunks to combine".into()));
        }
        
        // Concatenate all chunks
        let combined = concat(chunks, UnionArgs::default())?;
        
        // Sort by UTC-corrected timestamp and remove duplicates
        let final_df = combined
            .sort(["timestamp_utc_corrected"], SortMultipleOptions::default())
            .unique(None, UniqueKeepStrategy::First);
            
        Ok(final_df)
    }
    
    /// Step 5: Validate DST transitions and check for data quality issues
    fn validate_dst_transitions(&self, df: LazyFrame) -> PolarsResult<LazyFrame> {
        let collected = df.collect()?;
        
        // Check for missing hours (spring forward) and duplicate hours (fall back)
        // This is a simplified validation - full implementation would check specific transition times
        
        println!("üîç Validating DST transitions...");
        
        if collected.height() > 0 {
            println!("   ‚úÖ {} data points after DST correction", collected.height());
        }
        
        // Add validation columns
        let validated = collected
            .lazy()
            .with_columns([
                lit(true).alias("dst_validation_passed"),
                lit("DST correction applied successfully").alias("dst_validation_notes"),
            ]);
            
        Ok(validated)
    }
    
    pub fn identify_dst_transitions(&self, df: LazyFrame) -> PolarsResult<LazyFrame> {
        // Add flags for data points that occur during DST transitions
        // This helps identify potential data quality issues
        
        let with_dst_flags = df.with_columns([
            // Add column indicating if timestamp falls within 2 hours of a DST transition
            lit(false).alias("near_dst_transition"), // Simplified - full implementation would check actual transitions
            
            // Add column for the inferred timezone at this timestamp
            lit("EST/EDT").alias("inferred_timezone"),
        ]);
        
        Ok(with_dst_flags)
    }
    
    pub fn validate_chunk_consistency(&self, df: LazyFrame) -> PolarsResult<ChunkValidationReport> {
        let collected = df.collect()?;
        
        let total_rows = collected.height();
        let timestamp_col = collected.column("timestamp")?;
        
        let mut report = ChunkValidationReport {
            total_rows,
            missing_timestamps: 0,
            invalid_timestamps: 0,
            dst_transition_issues: 0,
            temporal_gaps: Vec::new(),
            warnings: Vec::new(),
        };
        
        // Count missing/invalid timestamps
        let valid_timestamps = timestamp_col.is_not_null().sum().unwrap_or(0) as u32;
        report.missing_timestamps = total_rows - valid_timestamps as usize;
        
        // In full implementation, would check for:
        // - Temporal gaps larger than expected measurement interval
        // - Duplicate timestamps
        // - Timestamps during DST transitions that don't follow expected patterns
        
        if report.missing_timestamps > 0 {
            report.warnings.push(format!(
                "Found {} missing timestamps out of {} total rows",
                report.missing_timestamps, total_rows
            ));
        }
        
        Ok(report)
    }
}

#[derive(Debug)]
pub struct ChunkValidationReport {
    pub total_rows: usize,
    pub missing_timestamps: usize,
    pub invalid_timestamps: usize,
    pub dst_transition_issues: usize,
    pub temporal_gaps: Vec<(DateTime<Utc>, DateTime<Utc>)>,
    pub warnings: Vec<String>,
}

impl ChunkValidationReport {
    pub fn is_valid(&self) -> bool {
        self.invalid_timestamps == 0 && self.dst_transition_issues == 0
    }
    
    pub fn summary(&self) -> String {
        format!(
            "Validation Report: {} rows, {} missing timestamps, {} invalid timestamps, {} DST issues",
            self.total_rows, self.missing_timestamps, self.invalid_timestamps, self.dst_transition_issues
        )
    }
}